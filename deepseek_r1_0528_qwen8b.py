# -*- coding: utf-8 -*-
"""DeepSeek-R1-0528-Qwen3-8B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B.ipynb
"""

# !pip install -U transformers

# """## MODELのインストール"""

# #GPUのキャッシュ
# !nvidia-smi

import os

# from google.colab import userdata
# os.environ["HF_TOKEN"] = userdata.get('HF_TOKEN')

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-0528-Qwen3-8B")
model = AutoModelForCausalLM.from_pretrained("deepseek-ai/DeepSeek-R1-0528-Qwen3-8B")
#推論モード
model.eval()

if torch.cuda.is_available():
  model.to("cuda")

#"prompt"
system_prompt = (
    "You are an expert mathematics tutor. "
    "Please generate a set of 5 challenging calculus problems (suitable for high-school to early-college level). "
    "For each problem, output a JSON object with keys:\n"
    "  - \"question\": the problem statement in English\n"
    "  - \"solution\": your chain-of-thought reasoning enclosed in <think>…</think> followed by the final answer.\n"
    "Return the entire output as a JSON array of these objects."
)

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user",   "content": "Generate the problem set now."}
]

# apply_chat_template が提供されている場合
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)

inputs = {k: v.to(model.device) for k,v in inputs.items()}

outputs = model.generate(
    **inputs,
    max_new_tokens=1024,
    do_sample=True,
    temperature=0.7,
    top_p=0.95,
    eos_token_id = tokenizer.eos_token_id,
    pad_token_id = tokenizer.pad_token_id,

)

raw = tokenizer.decode(
    outputs[0, inputs['input_ids'].shape[-1]:],
    skip_special_tokens=True,
    )

print(raw)